# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1oKXTow68tepikPLs4wYs0Zlq0d3YmWEo
"""

import pandas as pd
dataset=pd.read_csv("Fashion_data.csv")
dataset.drop(columns=['Unnamed: 16','Unnamed: 17','Unnamed: 18','Unnamed: 19','Unnamed: 20'], inplace=True)

#dataset.info()
dataset.drop(dataset[dataset['Product_Name']=='undefined'].index, inplace=True)
dataset.timestamp=pd.to_datetime(dataset.timestamp)
dataset['Weekday']=dataset['timestamp'].dt.day_name()
dataset['Date']=dataset['timestamp'].dt.date
dataset.Revenue=pd.to_numeric(dataset['Revenue'])
dataset.Product_Name.astype(str)
cleaned_dataset = dataset[dataset['Revenue']!=0]
cleaned_dataset=cleaned_dataset[~cleaned_dataset['Product_Name'].str.contains(',', na=False)]

cleaned_dataset=cleaned_dataset[~cleaned_dataset['Number_of_Products'].str.contains(',', na=False)]

###############################################################################################
#  Explode taking too much time to convert to rows

#dataset_new = Product.set_index(['user ID']).apply(lambda col:col.str.split(',').explode()).reset_index().reindex(Product.columns, axis=1)

'''
import matplotlib.pyplot as plt
import seaborn as sns
sns.set_theme()

group = dataset[['Weekday','Date','Revenue']].groupby(['Weekday','Date']).sum('Revenue')
#print(group.head(100))
revenue = group['Revenue'].groupby(level=0, group_keys=False)

gtp=revenue.nlargest(7)
week_dict = {"Monday":0,"Tuesday":1,"Wednesday":2,"Thursday":3,"Friday":4,"Saturday":5,"Sunday":6}
gtp.sort_index(level=0, key = lambda x:x.map(week_dict), inplace=True)

ax = gtp.plot(kind='bar')
plt.show()
count = gtp.groupby("Weekday").count()
cs = np.cumsum(count)
for i in range(len(count)):
    title = count.index.values[i]
    ax.axvline(cs[i]-.5, lw=0.8, color="k")
    ax.text(cs[i]-(count[i]+1)/2., 1.02, title, ha="center",transform=ax.get_xaxis_transform())
  
ax.set_xticklabels([l.get_text().split(", ")[1][:-1] for l in ax.get_xticklabels()])

plt.show()

import numpy as np
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity

Product.dropna(inplace=True)
top_5_most_selling_product=Product.groupby('Product_Name').agg({'user ID':'count','Revenue':'sum'}).nlargest(5,'user ID')
hot_products = list(top_5_most_selling_product.index)
#print(hot_products)
Final_Products =Product[Product["Product_Name"].isin(hot_products)]

#Final_Products.join(Final_Products)

print(Final_Products)
'''
'''
Product_Pivot = pd.pivot_table(Final_Products,values='Bought',index='user ID',columns='Product_Name')

#print(Product_Pivot.shape)

def recommend_similar(Product_Name, min_count=0):
    print("For Product ({})".format(Product_Name))
    print("- Top 10 products recommended based on Pearsons'R correlation - ")
    i = int(Final_Products.index[Final_Products['Product_Name'] == Product_Name][0])
    target = Product_Pivot[i]
    similar_to_target = Product_Pivot.corrwith(target)
    corr_target = pd.DataFrame(similar_to_target, columns = ['PearsonR'])
    corr_target.dropna(inplace = True)
    corr_target = corr_target.sort_values('PearsonR', ascending = False)
    corr_target.index = corr_target.index.map(int)
    corr_target = corr_target.join(Final_Products).join(cleaned_dataset_movie_summary)[['PearsonR', 'Name', 'count', 'mean']]
    print(corr_target[corr_target['count']>min_count][:10].to_string(index=False))

for i in hot_products:
  print(recommend_similar(i))
'''

from mlxtend.frequent_patterns import apriori
from mlxtend.frequent_patterns import association_rules

cleaned_dataset['Product Name'] = cleaned_dataset['Product_Name'].str.strip()
#Similarity basket
basket = (cleaned_dataset
          .groupby(['user ID', 'Product_Name'])['Number_of_Products']
          .sum().unstack().reset_index().fillna(0)
          .set_index('user ID'))
print(basket.head())

def encode_units(x):
    if int(x) <= 0:
        return 0
    if int(x) >= 1:
        return 1

basket_sets = basket.applymap(encode_units)
frequent_itemsets = apriori(basket_sets, min_support=0.01, use_colnames=True)
rules = association_rules(frequent_itemsets, metric="lift", min_threshold=1)

rules[ (rules['lift'] >= 6) &
       (rules['confidence'] >= 0.8) ]

rules.head()